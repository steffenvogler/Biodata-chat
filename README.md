# ğŸ§¬ BioData Chat

**Intelligent Terminal Interface for Scientific Database Interaction**

BioData Chat is a sophisticated terminal application that enables natural language interaction with multiple scientific databases through MCP (Model Context Protocol) servers, powered by local LLM models for privacy and efficiency.

## ğŸŒŸ Overview

This application provides seamless access to three major scientific databases:

- **ğŸ”¬ Bionomia Database** - Species attribution and collector information
- **ğŸŒ Encyclopedia of Life (EOL)** - Comprehensive species data, traits, and ecological interactions  
- **ğŸ›ï¸ CKAN Natural History Museum** - Museum datasets and collections

## âœ¨ Features

### ğŸ¨ User Experience
- **Rich Terminal Interface** - Beautiful, colorful CLI using the `rich` library
- **Interactive Chat Mode** - Natural language conversations with scientific data
- **Status Monitoring** - Real-time server status and health checks
- **Progress Indicators** - Visual feedback for long-running operations

### ğŸ”§ Technical Capabilities
- **MCP Server Integration** - Orchestrates multiple database servers automatically
- **Dual LLM Backend Support** - Primary llamafile backend with Ollama fallback for maximum compatibility
- **Resource Efficiency** - Optimized for local execution with lightweight models
- **Verbose Logging** - Detailed debugging information when needed
- **Error Handling** - Graceful error recovery and user-friendly messages

### ğŸ¯ Smart Features
- **Flexible Model Selection** - Choose from various LLM models via command-line arguments
- **Conversation History** - Maintains context across chat sessions
- **Runtime Model Switching** - Change LLM models on-the-fly during chat
- **Dependency Checking** - Automatic validation of required components
- **Server Management** - Integrated start/stop/restart functionality

## ğŸš€ Quick Start

### Prerequisites

1. **LLM Backend** - The application supports two backends:
   - **llamafile** (Primary) - Automatically downloaded lightweight LLM executable
   - **Ollama** (Fallback) - Install from [ollama.ai](https://ollama.ai) if llamafile fails
     ```bash
     # Only needed if llamafile doesn't work
     ollama serve
     ```

2. **FastMCP** - Should be available in your environment
3. **Python 3.8+** with pip

### Installation

#### Option 1: Development Container (Recommended)

The easiest way to get started is using the pre-configured development container:

1. **Prerequisites:**
   - [Docker](https://www.docker.com/get-started) installed
   - [VS Code](https://code.visualstudio.com/) with [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)

2. **Open in container:**
   ```bash
   git clone https://github.com/steffenvogler/Biodata-chat.git
   cd Biodata-chat
   code .
   ```
   Then click "Reopen in Container" when prompted.

3. **Everything is pre-configured:**
   - âœ… Python 3.11 with all dependencies
   - âœ… Ollama pre-installed and configured
   - âœ… Development tools ready
   - âœ… VS Code extensions loaded

#### Option 2: Local Installation

1. **Clone and navigate to the project directory:**
   ```bash
   cd /path/to/Biodata-chat
   ```

2. **Install initial minimal dependencies:**
   ```bash
   pip install -r requirements-minimal.txt
   ```

3. **Install full dependencies if needed:**
   ```bash
   pip install -r requirements.txt
   ```
   For machines with dependency issues, try only minimal requirements first.

4. **Make scripts executable:**
   ```bash
   chmod +x biodata_chat.py
   chmod +x manage_servers.sh
   ```

### Running the Application

1. **Basic usage:**
   ```bash
   ./biodata_chat.py
   ```

2. **With verbose logging:**
   ```bash
   ./biodata_chat.py --verbose
   ```

3. **Using a specific model:**
   ```bash
   ./biodata_chat.py --model "Custom Model Name"
   ```

4. **Backend selection with model:**
   ```bash
   ./biodata_chat.py --backend ollama --model "LLaMA 3.2 1B Instruct" --verbose
   ```

## ğŸ“– Usage Guide

### Starting a Chat Session

When you run the application, it will:

1. ğŸ” Check all dependencies
2. ğŸ¯ Configure the selected model for the chosen backend
3. ğŸ“¥ Download required LLM model (if needed)
4. ğŸš€ Start MCP servers automatically
5. ğŸ”— Initialize database connections
6. ğŸ’¬ Launch interactive chat mode

### Available Commands

| Command | Description |
|---------|-------------|
| `/help` | Show all available commands and usage tips |
| `/status` | Display current MCP server status |
| `/history` | Show recent conversation history |
| `/clear` | Clear conversation history |
| `/model <name>` | Switch to a different model (runtime) |
| `/verbose` | Toggle verbose logging on/off |
| `/quit` | Exit the application safely |

### Example Queries

Try asking questions like:

- ğŸ» *"Find information about polar bears"*
- ğŸŒ¸ *"Show me data about plant collectors"*
- ğŸ¦‹ *"What ecological interactions involve butterflies?"*
- ğŸ“Š *"Search for datasets about marine biodiversity"*
- ğŸ‘¨â€ğŸ”¬ *"Who collected specimens in Madagascar?"*

## ğŸ—ï¸ Architecture

### MCP Servers

The application manages three specialized MCP servers:

1. **Bionomia Server** (`bananompy_server.py`)
   - Collector search and attribution
   - Species occurrence data

2. **EOL Server** (`eol_server.py`)
   - Species information and media
   - Taxonomic hierarchies
   - Ecological traits and interactions

3. **CKAN Server** (`ckan_server.py`)
   - Dataset discovery
   - Collection browsing
   - Resource metadata

### Server Management

The `manage_servers.sh` script provides:
- **Start:** Launch all MCP servers
- **Stop:** Gracefully shut down servers  
- **Restart:** Stop and restart all servers
- **Pause:** Suspend server processes
- **Continue:** Resume suspended servers

## ğŸ”§ Configuration

### Model Selection

The application defaults to **"LLaMA 3.2 1B Instruct"** for optimal resource efficiency. The model name is intelligently mapped to the appropriate format for each backend:

- **llamafile backend**: `Llama-3.2-1B-Instruct.Q6_K.llamafile`
- **Ollama backend**: `llama3.2:1b`

#### Command-Line Model Selection

```bash
# Use default model (LLaMA 3.2 1B Instruct)
./biodata_chat.py

# Specify custom model
./biodata_chat.py --model "Custom Model Name"

# Use with specific backend
./biodata_chat.py --backend ollama --model "LLaMA 3.2 1B Instruct"

# Larger models (adjust based on your system capabilities)
./biodata_chat.py --model "LLaMA 3.2 3B Instruct"
./biodata_chat.py --model "CodeLlama 7B"
```

#### Available Options

| Option | Short | Description | Default |
|--------|--------|-------------|----------|
| `--model` | `-m` | Model name to use | "LLaMA 3.2 1B Instruct" |
| `--backend` | `-b` | Backend (llamafile/ollama) | llamafile |
| `--verbose` | `-v` | Enable detailed logging | False |

### Verbose Mode

Enable detailed logging for troubleshooting:

```bash
./biodata_chat.py --verbose
```

Or toggle during runtime with `/verbose` command.

## ğŸ› ï¸ Development

### File Structure

```
fastmcp/
â”œâ”€â”€ biodata_chat.py              # Main application
â”œâ”€â”€ manage_servers.sh            # Server orchestration script
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ src/fastmcp/
â”‚   â”œâ”€â”€ bananompy_server.py     # Bionomia MCP server
â”‚   â”œâ”€â”€ eol_server.py           # EOL MCP server
â”‚   â””â”€â”€ ckan_server.py          # CKAN MCP server
â””â”€â”€ BIODATA_CHAT_README.md      # This documentation
```

### Adding New Databases

To integrate additional scientific databases:

1. Create a new MCP server script in `src/fastmcp/`
2. Add server management to `manage_servers.sh`
3. Update the client initialization in `biodata_chat.py`
4. Extend the system prompt with database context

## ğŸ› Troubleshooting

### Common Issues

**Ollama not responding:**
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

**MCP servers won't start:**
```bash
# Check script permissions
chmod +x manage_servers.sh

# Test manually
./manage_servers.sh start
```

**Missing dependencies:**
```bash
# Reinstall requirements
pip install -r requirements.txt --force-reinstall
```

### Error Messages

The application provides detailed error messages with suggested solutions. Use `--verbose` flag for additional diagnostic information.

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- ğŸ”Œ Additional database integrations
- ğŸ¨ Enhanced UI components
- ğŸ§  Better query understanding
- ğŸ“Š Data visualization features
- ğŸ” Advanced search capabilities

## ğŸ“„ License

This project extends FastMCP and follows its licensing terms. Please refer to the FastMCP license for details.

## ğŸ™ Acknowledgments

- **FastMCP** - Provides the MCP server framework
- **Ollama** - Local LLM execution platform
- **Rich** - Beautiful terminal interfaces
- **Scientific Databases** - Bionomia, EOL, and CKAN for their open APIs

---

*Built with â¤ï¸ for the scientific research community*
