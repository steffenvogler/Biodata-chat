warming up the model with an empty run

llama server listening at http://10.211.12.105:8080
llama server listening at http://192.168.178.61:8080
llama server listening at http://127.0.0.1:8080

